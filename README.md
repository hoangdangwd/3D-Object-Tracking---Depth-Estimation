# CoTracker3: Point Tracking in Video

[![License](https://img.shields.io/badge/License-CC--BY--NC%204.0-blue)](LICENSE.md)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)

**CoTracker3** lÃ  mÃ´ hÃ¬nh AI theo dÃµi Ä‘iá»ƒm (point tracking) trong video, phÃ¡t triá»ƒn bá»Ÿi Meta AI Research vÃ  University of Oxford.

---

## ğŸ¯ TÃ­nh NÄƒng

- **Point Tracking**: Theo dÃµi báº¥t ká»³ Ä‘iá»ƒm nÃ o trong video (offline/online modes)
- **Dense Tracking**: Theo dÃµi grid lÃªn Ä‘áº¿n 265Ã—265 Ä‘iá»ƒm Ä‘á»“ng thá»i
- **3D Reconstruction**: TÃ­nh toÃ¡n tá»a Ä‘á»™ 3D tá»« tracking + depth estimation
- **Camera Motion**: Æ¯á»›c lÆ°á»£ng quá»¹ Ä‘áº¡o di chuyá»ƒn cá»§a camera
- **Real-time**: Há»— trá»£ webcam tracking

---

## ğŸ“¦ CÃ i Äáº·t

```bash
# Clone repository
git clone https://github.com/facebookresearch/co-tracker.git
cd co-tracker

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# CÃ i Ä‘áº·t package
pip install -e .
```

**Requirements:**
- Python â‰¥ 3.8
- PyTorch â‰¥ 2.0
- CUDA (optional, khuyáº¿n nghá»‹ cho GPU acceleration)

---

## ğŸš€ Pipeline Sá»­ Dá»¥ng Nhanh

### Mode 1: Full Pipeline (Tracking â†’ 3D â†’ Camera)
```bash
python pipeline/pipeline.py --video_path assets/apple.mp4 --mode full
```

### Mode 2: Tracking Only
```bash
python pipeline/pipeline.py --video_path video.mp4 --mode tracking --grid_size 20
```

### Mode 3: Webcam Real-time
```bash
python pipeline/pipeline.py --mode webcam
```

**Output:**
- `data/output/tracking/tracked_points.csv` - Tá»a Ä‘á»™ pixel tracked points
- `data/output/3d/points_3d.csv` - Tá»a Ä‘á»™ 3D (m) cá»§a cÃ¡c Ä‘iá»ƒm
- `data/output/velocity_3d.csv` - Váº­n tá»‘c thá»±c (m/s) cá»§a cÃ¡c Ä‘iá»ƒm
- `data/output/camera/camera_motion.csv` - Quá»¹ Ä‘áº¡o camera
- `data/output/videos/tracked_video.mp4` - Video visualization

---

## ğŸ“‚ Cáº¥u TrÃºc Dá»± Ãn

```
co-tracker-main/
â”œâ”€â”€ cotracker/              # Core package (models, datasets, evaluation)
â”œâ”€â”€ pipeline/               # Integrated pipeline
â”‚   â”œâ”€â”€ pipeline.py         # Main orchestrator
â”‚   â”œâ”€â”€ config.yaml         # Configuration
â”‚   â””â”€â”€ utils_pipeline.py   # Utilities
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ demos/              # Demo scripts
â”‚   â”œâ”€â”€ processing/         # Video processing
â”‚   â”œâ”€â”€ depth_3d/           # 3D reconstruction & depth
â”‚   â”œâ”€â”€ calibration/        # Camera calibration
â”‚   â””â”€â”€ training/           # Model training
â”œâ”€â”€ assets/                 # Sample videos & images
â”œâ”€â”€ models/                 # Model checkpoints
â”œâ”€â”€ data/                   # Input/Output data
â”œâ”€â”€ tools/                  # Quick start scripts
â””â”€â”€ docs/                   # Documentation
```

---

## ğŸ”§ Configuration

### Camera Calibration
Chá»‰nh sá»­a `pipeline/config.yaml`:

```yaml
camera_matrix:
  - [fx,  0, cx]
  - [ 0, fy, cy]
  - [ 0,  0,  1]

reference_points:  # (u, v, depth_meters)
  - [100, 800, 0.62]
  - [80, 750, 0.63]
```

**Láº¥y camera matrix:**
```bash
python scripts/calibration/calibrate_camera.py
```

### Kiá»ƒm Tra Depth Accuracy
**Test depth estimation vá»›i reference points:**
```bash
python scripts/depth_3d/test_depth_accuracy.py --image path/to/image.jpg
```

**Output:**
- Scale factor vá»›i confidence interval (mean Â± std)
- MAE/RMSE táº¡i reference points
- Outlier detection status
- Visualization: `data/output/depth_accuracy_test.png`

**TiÃªu chÃ­ cháº¥p nháº­n:**
- âœ… Scale factor std < 10% mean: Calibration tá»‘t
- âœ… MAE < 5cm: Äá»™ chÃ­nh xÃ¡c cao
- âœ… â‰¥3 valid points: ÄÃ¡ng tin cáº­y
- âš ï¸ Std > 20%: Cáº§n kiá»ƒm tra láº¡i reference points
- âŒ <3 points: KhÃ´ng thá»ƒ scale chÃ­nh xÃ¡c

---

## ğŸ“Š Models

### CoTracker Models

| Model | Checkpoint | Window | Description |
|-------|-----------|--------|-------------|
| CoTracker3 Offline | `models/cotracker.pth` | 60 frames | ChÃ­nh xÃ¡c cao, xá»­ lÃ½ offline |
| CoTracker3 Online | `models/cotracker_stride_4_wind_8.pth` | 16 frames | Real-time tracking |
| Scaled Offline | `models/scaled_offline.pth` | 60 frames | High-resolution tracking |

### Depth Model
- **MiDaS DPT_Large** (Intel ISL): Depth estimation tá»« monocular images

---

## ğŸ¨ Examples

### 1. Track Specific Points
```python
from cotracker.predictor import CoTrackerPredictor
import torch

model = CoTrackerPredictor(checkpoint="models/cotracker.pth")
video = torch.randn(1, 10, 3, 480, 640)  # [B, T, C, H, W]
queries = torch.tensor([[[0, 100, 200]]])  # [B, N, 3] (frame_idx, x, y)

pred_tracks, pred_visibility = model(video, queries=queries)
```

### 2. Dense Grid Tracking
```bash
python scripts/demos/demo.py --video_path video.mp4 --grid_size 50
```

### 3. 3D Coordinates from Single Image
```bash
python scripts/depth_3d/compute_3d_coords.py
```

### 4. Full Workflow with Custom Config
```bash
python pipeline/pipeline.py \
    --video_path video.mp4 \
    --mode full \
    --grid_size 20 \
    --output_dir results
```

---

## ğŸ§ª Scripts

### Demos (`scripts/demos/`)
- `demo.py` - Basic tracking demo
- `online_demo.py` - Online tracking mode
- `webcam_demo.py` - Webcam real-time tracking
- `demo_pipeline.py` - Interactive menu

### Processing (`scripts/processing/`)
- `extract_frames.py` - Extract frames tá»« video
- `track_video.py` - Track video vá»›i velocity calculation
- `test_tracker.py` - Test tracking accuracy

### Depth & 3D (`scripts/depth_3d/`)
- `compute_3d_coords.py` - TÃ­nh tá»a Ä‘á»™ 3D tá»« pixel + depth
- `compute_3d_relative.py` - Tá»a Ä‘á»™ 3D tÆ°Æ¡ng Ä‘á»‘i
- `compute_velocity_3d.py` - **TÃ­nh váº­n tá»‘c thá»±c (m/s) trong khÃ´ng gian 3D**
- `estimate_depth.py` - Depth estimation vá»›i MiDaS
- `test_depth_accuracy.py` - **Kiá»ƒm tra Ä‘á»™ chÃ­nh xÃ¡c depth estimation**
- `batch_3d_processing.py` - Xá»­ lÃ½ batch nhiá»u frames
- `estimate_camera_motion.py` - Æ¯á»›c lÆ°á»£ng camera pose
- `track_for_3d.py` - Track video Ä‘á»ƒ chuáº©n bá»‹ cho 3D processing

### Calibration (`scripts/calibration/`)
- `calibrate_camera.py` - Camera calibration vá»›i checkerboard

### Training (`scripts/training/`)
- `train_kubric.py` - Train trÃªn Kubric dataset
- `train_on_real_data.py` - Fine-tune trÃªn real videos

---

## ğŸ“ Mathematical Formulas

### 1. Pixel to 3D Projection
```
Given: pixel (u, v), depth Z, camera matrix K = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]

X = (u - cx) Ã— Z / fx
Y = (v - cy) Ã— Z / fy
Z = Z
```

### 2. Depth Scaling
```
# MiDaS returns disparity (inverse depth)
disparity = MiDaS(image)
depth_inv = 1 / (disparity + Îµ)  # Îµ = 1e-6 Ä‘á»ƒ trÃ¡nh chia cho 0

# Scale tá»« reference points vá»›i outlier removal
scales = [Z_real / depth_inv[v, u] for (u, v, Z_real) in reference_points]
median = median(scales)
MAD = median(|scales - median|)
scales_filtered = [s for s in scales if |s - median| < 2*MAD]
scale_factor = mean(scales_filtered)

depth_real = depth_inv Ã— scale_factor
```

**Critical Notes:**
- âœ… MiDaS tráº£ vá» **disparity**, khÃ´ng pháº£i depth trá»±c tiáº¿p
- âœ… Pháº£i dÃ¹ng `1/(disparity + Îµ)` Ä‘á»ƒ convert sang metric depth
- âœ… Scale factor cáº§n â‰¥3 reference points há»£p lá»‡
- âœ… Outlier removal dÃ¹ng MAD (Median Absolute Deviation)
- âœ… Sub-pixel depth dÃ¹ng bilinear interpolation

### 3. Rigid Transform (Camera Motion)
```
Given: 3D points A (frame t), B (frame t+1)

Centroid: c_A = mean(A), c_B = mean(B)
Centered: A' = A - c_A, B' = B - c_B
Covariance: H = A'^T Ã— B'
SVD: U, S, V^T = svd(H)
Rotation: R = V Ã— U^T
Translation: t = c_B - R Ã— c_A
```

---

## ğŸ”¬ Technical Details

### CoTracker Architecture
- **Backbone**: Vision Transformer (ViT)
- **Temporal Context**: Sliding window (60 frames offline, 16 frames online)
- **Output**: Tracks shape `[B, T, N, 2]`, Visibility `[B, T, N]`

### Pipeline Flow
```
Video (TÃ—HÃ—WÃ—3) 
  â†’ CoTracker â†’ Tracks (TÃ—NÃ—2) pixel coords
  â†’ MiDaS â†’ Disparity (TÃ—HÃ—W) â†’ depth_inv = 1/(disparity+Îµ)
  â†’ Scale with reference points â†’ Depth_real (TÃ—HÃ—W) meters
  â†’ Bilinear interpolation â†’ Points_3D (TÃ—NÃ—3) meters
  â†’ Velocity Calculation â†’ Velocity (TÃ—N) m/s
  â†’ Rigid Transform â†’ Camera_pose (TÃ—3)
```

**Depth Estimation Quality Checks:**
- âœ… Scale factor std < 10% cá»§a mean â†’ Good calibration
- âœ… MAE < 5cm at reference points â†’ Accurate
- âœ… â‰¥3 valid reference points â†’ Reliable
- âš ï¸ Scale factor std > 20% â†’ Check reference points
- âŒ <3 valid points â†’ Cannot scale reliably

---

## ğŸ“Š Performance

| Model | TAP-Vid-DAVIS J&F | FPS (GPU) | Memory |
|-------|-------------------|-----------|--------|
| CoTracker3 Offline | 77.3 | 25 | 11 GB |
| CoTracker3 Online | 71.2 | 60 | 6 GB |

**Hardware**: NVIDIA RTX 3090, Video 480Ã—640

---

## ğŸ› ï¸ Quick Start Tools

```bash
# Windows
cd tools
quick_start.bat

# Linux/Mac
cd tools
./quick_start.sh
```

---

## ğŸ“ Data Format

### Tracked Points CSV
```csv
Frame,Query,X,Y,Visibility
0,0,320.5,240.2,1.0
0,1,450.1,180.7,1.0
1,0,322.1,241.0,1.0
```

### 3D Points CSV
```csv
Frame,Query,X_m,Y_m,Z_m
0,0,0.125,-0.083,0.750
0,1

### Velocity 3D CSV
```csv
Query Index,Frame,X (m),Y (m),Z (m),Distance (m),Velocity (m/s),Velocity_X (m/s),Velocity_Y (m/s),Velocity_Z (m/s)
0,1,0.126,-0.082,0.751,0.0025,0.075,0.003,-0.001,0.001
0,2,0.128,-0.081,0.753,0.0032,0.096,0.006,0.003,0.006
```,0.201,-0.142,0.755
```

### Camera Motion CSV
```csv
Frame,Camera_X,Camera_Y,Camera_Z,Distance_m
0,0.000,0.000,0.000,0.000
1,0.005,-0.002,0.010,0.011
```

---

## ğŸ”— References

### Papers
- **CoTracker3** (2024): "CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos"
- **CoTracker** (2023): "CoTracker: It is Better to Track Together"

### Links
- GitHub: https://github.com/facebookresearch/co-tracker
- Project Page: https://co-tracker.github.io
- PyTorch Hub: `torch.hub.load("facebookresearch/co-tracker", "cotracker3_online")`

---

## ğŸ“„ License

CC-BY-NC 4.0 License. Xem [LICENSE.md](LICENSE.md) Ä‘á»ƒ biáº¿t chi tiáº¿t.

**Developed by:**
- Meta AI Research (FAIR)
- University of Oxford - Visual Geometry Group

---

## ğŸ™ Acknowledgments

- **MiDaS**: Intel ISL depth estimation model
- **TAPNet**: TAP-Vid benchmark datasets
- **Kubric**: Synthetic video generation

---

## ğŸ“® Contact & Support

- **Issues**: https://github.com/facebookresearch/co-tracker/issues
- **Discussions**: https://github.com/facebookresearch/co-tracker/discussions
- **Citation**:
```bibtex
@article{karaev2024cotracker3,
  title={CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos},
  author={Karaev, Nikita and Rocco, Ignacio and Graham, Benjamin and Neverova, Natalia and Vedaldi, Andrea and Rupprecht, Christian},
  journal={arXiv preprint arXiv:2410.11831},
  year={2024}
}
```
